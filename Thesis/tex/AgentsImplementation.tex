\chapter{Implementation of the Agents}
\label{implOfAgents}
In this project, the reinforcement learning (RL) agents agents are designed such that their functionality is encapsulated within a top-level scene called \texttt{Main.tscn}. Within this scene, there is an instance of the selected agent, and the code makes use of several functions implemented by the agent in order to interact with the environment. Specifically, the required functions include: \texttt{move()}, \texttt{init()}, \texttt{start\_game()}, \texttt{end\_game()}, \texttt{save()}.

The purpose of most of these functions is self-explanatory. \texttt{init()} and \texttt{save()} are used to initialize and save the agent's internal state, respectively, and are called only once per experiment. \texttt{start\_game()} and \texttt{end\_game()} are called at the beginning and end of each episode, while \texttt{move()} is called by the \texttt{Tunnels.gd} script, and it is in this function that the agent makes a decision about which action to take based on the current state and score.

\section{Hierarchy}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{agents_tree}
    \caption{Agents hierarchy inside the project}
    \label{fig:agents_tree}
\end{figure}

In the current design of the game, there are a total of 8 agents implemented, 5 of which utilize some form of reinforcement learning algorithm. These RL agents share a common superclass called \texttt{LearningAgent}, while 3 of them are further subclasses under the \texttt{TDAgent} class (see Figure \ref{fig:agents_tree}). As previously discussed, the RL algorithms can be broadly divided into two categories: Monte Carlo methods and temporal difference (TD) learning. The TD algorithms differ only in their update function, and so it was deemed appropriate to group them under the same superclass. However, the Double Q-Learning agent, which uses separate policies and requires additional modifications, was implemented as a separate subclass of the \texttt{LearningAgent}. The implementation details of these agents will be further elaborated upon in the subsequent sections. 

As previously mentioned when describing the state in Section \ref{state_descr}, the agent has the ability to perform movements in various directions, including moving right, forward, or left, and each of these movements can be performed in combination with shooting. Therefore, on each time step, the move function of the agent returns a list of two elements: the first element signifies the movement direction, where a value of -1 represents left, 0 represents forward, and 1 represents right; while the second element determines whether the agent will shoot or not, with a value of 1 indicating yes and 0 indicating no for shooting.


\section{Simple Agents}
To facilitate testing of the game environment, several simple agents were implemented. These agents serve as baseline models and were used to ensure that the environment was functioning as intended before more sophisticated RL agents were developed. There are three simple agents in total: a ``Keyboard agent'' that receives input from the player via the keyboard, a ``Static agent'' that always chooses the forward action without shooting, and a ``Random agent'' that chooses a random action at each time step. 

\section{Learning Agent}
This class serves as a base class for all the reinforcement learning agents in this project. It provides a set of shared functions and features that are used by all agents, such as reading and writing data to a file and debugging statements. In terms of decision-making, these agents all follow an $\epsilon$-greedy policy, whereby they select the action with the highest value for a given state with a certain probability, or randomly choose any action with the remaining probability. Each of the subclasses of the \texttt{LearningAgent} class then implements specific code that is unique to that particular agent.

\begin{center}
\hrulefill
\begin{lstlisting}
func choose_action(action):
    epsilon_action = false
    if not is_eval_game: # Used for continuous evaluation
        epsilon_action = rand.randf_range(0,1) < EPSILON
        if epsilon_action:
           action = ACTIONS[rand.randi_range(0,len(ACTIONS) - 1)]  
    return action
\end{lstlisting}
\hrulefill
\end{center}

\subsection{Common parameters and behaviours}
\label{com_par_and_beh}
In regards to the present implementation of the reinforcement learning algorithms for the 3D tunnel game, certain aspects of the game learning step and parameter implementation are unique to this project and merit discussion. For instance, we should look into the learning step in the game. The agent will not request a new move until the state has changed, despite the fact that it may seem more natural for a new decision to be made on every game tick. This has the effect of reducing the number of decisions that the agent must make in a given episode, but also results in intriguing policy behaviours that are elaborated upon in Chapter \ref{experiments_chapter}.

Furthermore, there are several parameters that are common to all learning agents, some of which were briefly discussed in the preceding chapter. In this section, we will examine in more detail how these parameters were integrated into this particular project. The initial optimistic value parameter is the simplest one to explain, as it is implemented in a straightforward manner. In particular, each time a state-action pair is added to the policy, its value is set to a predetermined number. This number (\texttt{initOptVal}), along with the other agent's sub-options parameters mentioned subsequently in this subsection, are specified through the command line (see \ref{commOpt}).

The following two parameters worth noting are \texttt{eps} and \texttt{epsFinal}, which are responsible for the random moves executed by the $\epsilon$-greedy policy. These parameters allow the user to specify the starting and ending values of $\epsilon$. Then, at the end of each game, the new $\epsilon$ value is calculated by multiplying the current $\epsilon$ value with the $decrease$ which is computed as follows\footnote{This computation is performed once, when initializing the agent.}: $$decrease = (\frac{epsFinal}{eps})^{\frac{1.0}{n}}$$

Here, \texttt{n} represents the number of games being played. The reason behind this epsilon decrease is to change the ratio between exploration and exploitation over time. At the beginning of the experiment, the \texttt{eps} value is higher, and thus random moves happen more often, causing the agent to try actions it would otherwise oversee. Later, when the policy is a bit stabilized, the \texttt{eps} value becomes smaller so it would allow the agent to play longer games and possibly win (if the \texttt{eps} value was high throughout the whole experiment, the agent would have a bigger chance of choosing an inadequate move and thus untimely ending the game).

Finally, it is pertinent to discuss the discounting value $\gamma$ (defined by \texttt{gam}). In this project, discounting is used in the following manner: $$\gamma^{next\_step.time - curr\_step.time}$$

In many reinforcement learning environments, all learning steps have the same duration, so each reward is discounted by $\gamma^{n}$, where \texttt{n} is the number of steps that elapsed between the time when an action was taken and the time when the reward was received.  And so in a typical implementation that iterates over the steps in an episode, the accumulated discount rate is multiplied by $\gamma$ in each iteration. However, as previously stated, learning steps in this implementation do not occur on every tick, but instead occur when the state changes. As a result, they may vary in size. To avoid uneven discounting, the time (in milliseconds) is calculated for each new decision made by the agent using this formula:  $$(game.num\_of\_ticks * 33) / 1000.0$$

\subsection{Monte Carlo Agent}
The Monte Carlo method is a type of reinforcement learning algorithm that updates its policy only after an episode is completed. This is done by iterating through the entire episode, going from the last step towards the first, and increasing the number of visits and total return for each state-action pair, if this is their first visit inside this episode. The total return is calculated using the formula shown in the code above, while the number of visits is simply incremented by 1. To determine the optimal action, the agent compares the ratio of total return to number of visits for each possible action at a given state\footnote{The pseudocode in Algorithm \ref{algo:MC} keeps a list of all returns for each state/action pair, while in our implementation a return for a particular state/action pair is calculated with the mentioned ratio.}. This calculation is performed at each state transition during the episode. To clarify, instead of calculating a new move each time the \texttt{move()} function is called, the agents will always choose the same action based on the current state. Only once the state has changed, the new action is chosen based on the accumulated score and the new state. This implementation has resulted in a certain behaviour of the agents which will be more discussed in Section \ref{intbeh}.
As previously mentioned, the $\gamma$ constant in the equation shown in the code serves as a discount factor, meaning that the last move made, which resulted in termination of the game, will receive the highest penalty. As we move further down the list of moves, their significance decreases. It is important to note that if the value of $\gamma$ is set to 1, all moves are given equal weight. 

\begin{center}
\hrulefill
\begin{lstlisting}
# MonteCarlo agent update
var R = (next_step.score - curr_step.score)
G = pow(GAMMA,next_step.time - curr_step.time) * (R + G)
            
# since we are using the first visit approach,
# we only need the first occurrence  of this state_action
if is_first_occurrence (...):
    total_return[curr_step.state_action] += G
    visits[curr_step.state_action] += 1
\end{lstlisting}
\hrulefill
\end{center}

\subsection{TD Agent}	
Unlike the Monte Carlo methods, which update their policies only after the completion of an episode, TD agents update their policies in real time, after each action is taken. To accomplish this, all TD agents have a shared function called \texttt{move()}, which calls the function displayed in the code provided below. 

\begin{center}
\hrulefill
\begin{lstlisting}
# TD agents update
visits[last_state_action] += 1
var alpha = 1.0 / visits[last_state_action]
var new_state_val = 0 if terminal else new_state_action
var new_gamma = pow(GAMMA,curr_time - prev_time)
q[last_state_action] += alpha * (new_gamma * (R + new_state_val) - q[last_state_action])
\end{lstlisting}
\hrulefill
\end{center}

The update of the policy for a specific state value in TD learning involves multiple variables, most of which have been previously discussed. One new variable is $\alpha$, which represents one devided by the total number of visits for a given state action pair. Furthermore, to make this calculation, a variable uniquely computed by each TD algorithm, known as \texttt{new\_state\_action}, is required. Various methods for computing this variable can be found in the code-snippets below. If the current state is a terminal state, the \texttt{new\_state\_action} will not be used and instead, it will be replaced with a value of 0, as indicated in the update code.

\begin{center}
\hrulefill
\begin{lstlisting}
# SARSA agent new_state_action variable calculation
func get_update(state, new_action, _best_action):
    return q[get_state_action(state, new_action)]
\end{lstlisting}
\hrulefill
\end{center}

\begin{center}
\hrulefill
\begin{lstlisting}
# QLearning agent new_state_action variable calculation
func get_update(state, _new_action, best_action):
    return q[get_state_action(state, best_action)]
\end{lstlisting}
\hrulefill
\end{center}

\begin{center}
\hrulefill
\begin{lstlisting}
# ExpectedSARSA agent new_state_action variable calculation
func get_update(state, _new_action, best_action):
    var sum = 0.0 
    # Calculate and return the expected value 
    for action in ACTIONS:
        var probability = EPSILON / len(ACTIONS)
        if action == best_action:
            probability += 1 - EPSILON
        sum += probability * Q(state,action)
    return sum
\end{lstlisting}
\hrulefill
\end{center}

In SARSA, the \texttt{new\_state\_val} is calculated based on the value of the next action the agent will take, denoted as \texttt{new\_action}. On the other hand, Q-learning uses the value of the best action possible in the next state, denoted as \texttt{best\_action}. These two variables are equal if a greedy policy is implemented. However if we consider a $\epsilon$-greedy policy, then they might differ based on whether a random action has been chosen. Expected SARSA combines these two approaches by taking the expected value of all possible actions in the next state. 

Similar to the agents in the TD class, the update for the Double Q-Learning agent occurs each time the agent changes its state. The update process is slightly different. In this method, two separate action-value functions, denoted as \texttt{q1} and \texttt{q2}, are used to estimate the maximum action value for a given state. At each update step, one of the Q-values is selected randomly and updated using the other one as a reference. This process helps to reduce the overestimation of action values and leads to more stable learning.

\begin{center}
\hrulefill
\begin{lstlisting}
# DoubleQLearning agent update
visits[last_state_action] += 1
var alpha = 1.0 / visits[last_state_action]
var new_gamma = pow(GAMMA, curr_time - prev_time)

if rand.randf_range(0,1) < 0.5:
    var new_state_val = 0 if terminal else
        q2[get_state_action(state,.best_action(state,q))]
    q[last_state_action] += alpha * (new_gamma * (R + new_state_val) -q[last_state_action])
else:
    var new_state_val = 0 if terminal else
        q[get_state_action(state,.best_action(state,q2))]
    q2[last_state_action] += alpha * (new_gamma * (R + new_state_val) -q2[last_state_action])
\end{lstlisting}
\hrulefill
\end{center}