\chapter{Structure of the Experimental Setting}
\label{agent_code_chapter}
To train an agent on a specific environment, the user must utilize a command-line interface. There are various options available to cater to the user's needs. This chapter will outline all of the provided options and how they are encoded. However, we will first consider how the game represents discrete states.

\section{State}
The majority of the implemented agents in the game utilize the concept of state to facilitate learning. The agent will determine its next action based on the current state in which it finds itself. By dividing the game into discrete states, we are able to utilize tabular method algorithms to train an agent to play this continuous game. Once one obstacle is passed, the value of the state indicated refers to the next one. Each state is represented by a triplet consisting of distance, rotation, and next obstacle type. 

Before further explaining this notion, lets look at the standard unit of distance measurement in Godot - a ``meter'' .It is used to represent the size, position, and movement of objects in the game world. In Godot, one meter is equal to the size of a standard cube. To better understand the notion, it would be useful to note that Hans is approximately 12m tall and his starting speed is 35 meters per second, while each tunnel has width and height of approximately 45m and depth 2800m.

The distance value indicates the distance of the agent from the next obstacle in meters. For example, if we set the \texttt{dists} parameter (indicated by the user, see \ref{opt:dists}) to 2, there are two possible distance values for the state: \textgreater 50, \textgreater 0, indicating that the agent is more than 50 meters away from the obstacle and at least 1-50 meter away from the obstacle, respectively. If the \texttt{dists} parameter is set to 1, the distance value remains constant at \textgreater 0. In the Chapter \ref{experiments_chapter} we will see that all types of environments in this game are able to learn when the \texttt{dists} parameter is equal to 1.  

The rotation parameter divides the obstacle circumference into \texttt{rots} number of intervals (see \ref{opt:rots}). As the tunnel rotates, the state label will indicate the rotation value to which Hans is aligned at the moment (in Figure \ref{fig:circumference} Hans is aligned with rotation value 240). If the \texttt{rots} parameter is set to 360, this would correspond to the number of degrees in a circle and result in 360 possible rotations. However, the obstacles in this game do not require such a high number of rotations and agents can be trained to avoid most obstacles using fewer than 10 rotations. 

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{circumference}
    \caption{Rotations when \texttt{rots = 6}}
    \label{fig:circumference}
\end{figure}

Finally, the type parameter indicates the type of obstacle that Hans must avoid. Each obstacle has its own string representation. With this information, the agent can learn to recognize the safe rotation for different types of obstacles and, along with the distance parameter, decide whether to move left, right, forward, or shoot in combination with any of these actions. To further explain this notion lets look back at the Figure \ref{fig:circumference}. Here, seemingly the 240 rotation value should be the way to go and Hans can easily pass through the obstacle. However, with closer observation, it is visible that in fact, not the whole of rotation 240 is safe. The agent does not know the difference between being at any point of the 240 rotation and thus can easily die. For this obstacle, indeed, there would be more \texttt{rots} needed in order for Hans to have at least one safe rotation value (meaning that the whole piece of the obstacle that rotation value covers is considered safe).

\section{Command line options}
\label{commOpt}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ | p{4cm}|p{10cm} | } 
  \hline+
  \hyperref[opt:n]{n=int} & number of games\\ 
  \hline
  \hyperref[opt:agent]{agent=string} & name of the agent\\ 
  \hline
  \hyperref[opt:level]{level=int} & number of the level to start from \\ 
  \hline
  \hyperref[opt:env]{env=[string]} & list of obstacles that will be chosen in the game \\ 
  \hline
  \hyperref[opt:shooting]{shooting=string} & enable or disable shooting  \\ 
  \hline
  \hyperref[opt:dists]{dists=int} & number of states in a 100-meter interval \\ 
  \hline
  \hyperref[opt:rots]{rots=int} & number of states in 360 degrees rotation \\ 
  \hline
  \hyperref[opt:database]{database=string} & read the data for this command from an existing file and/or update the data after the command is executed  \\ 
  \hline
  \hyperref[opt:ceval]{ceval=bool} & performs continuous evaluation \\ 
  \hline
  \hyperref[opt:debug]{debug=bool} & display debug print statements \\ 
  \hline
  \hyperref[opt:options]{options} & displays options \\ 
  \hline
\end{tabular}}
\end{center}
Note: any of these options can be omitted as they all have default values. If no options are specified, a normal game with the \texttt{Keyboard} agent will be run.
\subsection{Command line option descriptions}
\begin{description}
\item[n] - Number of games the agent will train on in this session. The default is 100. \label{opt:n}
\item[agent] - Name of the desired agent \label{opt:agent}\\
Options: [\texttt{Keyboard}, \texttt{Static}, \texttt{Random}, \texttt{MonteCarlo}, \texttt{SARSA}, \texttt{QLearning}, \\ \texttt{ExpectedSARSA}, \texttt{DoubleQLearning}]\\
Sub-options (only for the listed agents):
\texttt{MonteCarlo}, \texttt{SARSA}, \texttt{QLearning}, \texttt{ExpectedSARSA}, \texttt{DoubleQLearning}
=[float, float, float, float] :

[\texttt{gam} (range [0,1]), \texttt{eps} (range [0,1]), \texttt{epsFinal} (range [0,1]), \texttt{initOptVal} (range [0,$\infty$))]\\
Example usage: ``\texttt{agent=MonteCarlo:eps=0.1,gam=0.2}''\\
The meaning of the suboptions will be explained in a later section.
\item[level] - Number of the level to start from. Default value is 1. \label{opt:level}\\
Options: \texttt{[1, ... , 10]}\\
Note: after the 10th level, the agent is considered to have won the game
\item[env] - List of obstacles that will be chosen in the game \label{opt:env}\\
Options (any subset of): [\texttt{Traps}, \texttt{Bugs}, \texttt{Viruses}, \texttt{Tokens},
\texttt{I}, \texttt{O}, \texttt{MovingI}, \texttt{X}, \texttt{Walls}, \texttt{Hex},
\texttt{HexO}, \texttt{Balls}, \texttt{Triangles}, \texttt{HalfHex},
\texttt{Worm}, \texttt{LadybugFlying}, \texttt{LadybugWalking},
\texttt{Rotavirus}, \texttt{Bacteriophage}]\\
Note: if this parameter is not included, the environment will contain all available obstacles (i.e. the full game).\\
Example usage: ``\texttt{env=HexO,I,Bugs}''\\
\item[shooting] - Enable or disable shooting \label{opt:shooting}\\
Options: [\texttt{enabled}, \texttt{disabled}]\\
Note: this option is disabled by default.
\item[dists] - Number of states in a 100-meter interval \label{opt:dists}\\
This parameter is part of the state label and typical options range from 1 to 3. Default value is 1.
\item[rots] - Number of states in 360 degrees rotation \label{opt:rots}\\
This parameter is part of the state label and the minimum viable option is 6. This is also the default value.
\item[database] - Read or write data for this command from/to a file \label{opt:database}\\
Options: [\texttt{read}, \texttt{write}, \texttt{read\_write}]\\
Note: these files are typically used to start another session of the agent's training from the last point of the previous session, to run a game with visuals and observe the agent's performance, or for plotting the results. This option does not affect the Keyboard, Static, or Random agents. Default option for this parameter is to neither read nor write.
\item[ceval] - Performs continuous evaluation \label{opt:ceval}\\
This parameter indicates that after each training game, a test game will be played using only the policy(s) learned thus far. For example, if the user specifies ``n=100'', a total of 200 games will be executed, with 100 of them being training games and the remaining 100 being test games. This allows for the assessment of the agent's progress and performance during the training process. 
Options: [\texttt{true},\texttt{false} (default)]
\item[debug] - Display debug print statements \label{opt:debug}\\
Options: [\texttt{true},\texttt{false} (default)]
\item[options] - Displays all of the mentioned options \label{opt:options}\\
\end{description}

\subsection{Running the program}
There are several possibilities for running the game from the command line. In addition to various combinations of the options listed above, the user has the choice of running an experiment with or without the graphical interface. If they opt for the first possibility, the window will open and the game will be played at its normal speed. On the other hand, if the experiment is run without graphics, it will be over 200 times faster and the output will only be displayed in the terminal. This is achieved by a few things, mainly hiding the CSG geometry in every node. The computational power required to perform union, intersection, etc. on the CSG shapes is quite high and thus by not performing those calculations a game can run much faster. Of course, these shapes are not necessary for the experiments run without graphics, since the collision shapes are the ones that play a role in determining what happened in the game\footnote{The collision shapes refer to the shapes that are used to define the physical bounds of an object for the purpose of collision detection.}.
It should also be noted that the experiments are fully reproducible since the seed values for all random variables are predetermined.

To run the program in the command line, the user may benefit from adding the Godot executable to the PATH environment variable. This will allow them to start the application from the command line simply by entering the command \texttt{godot} while inside the same directory as the \texttt{project.godot} file.

By default, running the program in this manner will launch a normal game with the graphical interface and the \texttt{Keyboard} agent. However, the user can customize their experiment by using a combination of the options listed above. For example:

\begin{algorithm}
\begin{algorithmic}
\State godot database=write agent=SARSA:initOptVal=100.0,eps=0.3 env=HexO n=10 dists=1 rots=8
\end{algorithmic}
\end{algorithm}

Alternatively, the user may choose to train the agent faster by disabling the graphical interface and increasing the speed of the program. This can be achieved by modifying the previous command as follows:

\begin{algorithm}
\begin{algorithmic}
\State godot --no-window --fixed-fps 1 --disable-render-loop database=write agent=SARSA:initOptVal=100.0,eps=0.3 env=HexO n=10 dists=1 rots=8
\end{algorithmic}
\end{algorithm}

To view a list of available options, the user can simply enter the command \texttt{godot options}.

\section{Main}
The \texttt{Main.tscn} scene is the top level scene in the game and consists of a single Node type node. The script attached to this node, \texttt{Main.gd}, is responsible for ensuring that all options specified in the command line (as discussed in Section \ref{commOpt}) are executed correctly. This script is the starting point of the training and handles the initialization and execution of ore or more game sessions.
There are several key functions within the \texttt{Main.gd} script that are worth discussing in more detail.

\begin{algorithm}
\begin{algorithmic}[1]
\Function{\_ready}{}
\State $unparsed\_args \gets OS.\text{get\_cmdline\_args()}$
\If{$unparsed\_args.size() = 1$ and $unparsed\_args[0] = "options"$}
\State \Call{display\_options}{}
\EndIf
\State $\dots$ \Comment{parse args}
\If{\Call{set\_param}{args} $= \text{false}$}
\State \Call{display\_options}{}
\Else
\State \Call{instance\_agent}{}
\State \Call{build\_filename}{}
\If{\textbf{not} $agent\_inst.\text{init}(actions, read, write, command, n, debug)$}
\State \Call{print}{``Something went wrong, please try again''}
\State \Call{display\_options}{}
\EndIf
\State \Call{play\_game}{}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

The \texttt{\_ready()} function is the starting point of the program when run from the command line. It is responsible for parsing all of the arguments and checking their validity. If any issues are encountered, the program will display options and terminate. If the arguments are valid, an agent will be instantiated and initialized. In cases where everything is in order, first game will be played by calling the \texttt{play\_game()} function.

\begin{algorithm}
\begin{algorithmic}[1]
\Function{play\_game}{}
\If{agent $=$ "Keyboard" and VisualServer.render\_loop\_enabled}
\State $\dots$ \Comment{play a regular game}
\ElsIf{$n > 0$}
\State $n \gets n - 1$
\State $game \gets$ $game\_scene.\text{instance}()$
\State \Call{set\_param\_in\_game()}{}
\State $agent\_inst.\text{start\_game}(is\_eval\_game)$
\Else
\State $agent\_inst.\text{save}(write)$
\State \Call{print\_and\_write\_ending()}{}
\EndIf
\EndFunction
\end{algorithmic}
\end{algorithm}

The \texttt{play\_game()} function is called each time a game is played.Firstly, it will check if the agent selected was the \texttt{Keyboard} agent, and if so, the program will start one game session where the user has controls of Hans. Otherwise, one of the remaining agents will take over and play specified number of games (defined by the "n" parameter). If n games have already been played, the program will terminate after performing the last few tasks needed to save all of the knowledge gained form this particular session. Otherwise, a single game will be executed and number of games left decreased.

\begin{algorithm}
\begin{algorithmic}[1]
\Function{on\_game\_finished}{$score, ticks, win, time$}
    \State \Call{print\_and\_write\_score}{score, win}
    \State $agent\_inst.\text{end\_game}(score, time)$
    \State \Call{play\_game()}{}
\EndFunction
\end{algorithmic}
\end{algorithm}

The \texttt{game\_over()} function is called when the game emits a signal indicating that it has finished. Upon execution, this function outputs the necessary information, updates the agent through the \texttt{end\_game()} function, and then calls the \texttt{play\_game()} function to continue the game session.