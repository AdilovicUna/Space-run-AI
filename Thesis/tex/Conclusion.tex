\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

In conclusion, this thesis has examined the performance of different reinforcement learning algorithms on various environments in the Godot game engine.  The results showed that the Expected SARSA algorithms performed consistently well multiple environments, while the Double Q-Learning algorithm struggled with some but improved with more training. All algorithms performed quite well with the individual traps, while with all trap types combined, Monte Carlo and once again Expected SARSA gave the best results. In the token environment, all algorithms performed well. A very interesting observation is that in all cases that include bugs and viruses, the agents chose to rather avoid the obstacle than shoot it down. Unfortunately, none of the algorithms were able to achieve optimal performance on the full game. Further research is needed to determine the optimal parameters and strategies for these algorithms to accomplish that task.